# %% [markdown]
# # –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ ML –º–æ–¥–µ–ª–µ–π
#
# ## –°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ:
# 1. –ó–∞–≥—Ä—É–∑–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
# 2. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
# 3. –ê–Ω–∞–ª–∏–∑ –æ—à–∏–±–æ–∫
# 4. –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
# 5. –í—ã–≤–æ–¥—ã

# %% [markdown]
# ## 1. –ò–º–ø–æ—Ä—Ç –±–∏–±–ª–∏–æ—Ç–µ–∫

# %%
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import json
import os
from IPython.display import display, Markdown

plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette("husl")

# %% [markdown]
# ## 2. –ó–∞–≥—Ä—É–∑–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

# %%
# –ù–∞—Ö–æ–¥–∏–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —Ñ–∞–π–ª —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
results_dir = '../results/ml_results'
if not os.path.exists(results_dir):
    print("Results directory not found!")
else:
    # –ò—â–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π JSON —Ñ–∞–π–ª
    json_files = [f for f in os.listdir(results_dir) if f.endswith('.json')]
    if json_files:
        latest_file = max(json_files, key=lambda x: os.path.getctime(os.path.join(results_dir, x)))
        results_path = os.path.join(results_dir, latest_file)

        with open(results_path, 'r') as f:
            results = json.load(f)

        print(f"Loaded results from: {latest_file}")
        print(f"Models tested: {list(results.keys())}")
    else:
        print("No results files found!")

# %% [markdown]
# ## 3. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π

# %%
if 'results' in locals():
    # –°–æ–∑–¥–∞–µ–º DataFrame —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏
    metrics_data = []
    for model_name, model_results in results.items():
        metrics_data.append({
            'Model': model_name.replace('_', ' ').title(),
            'Accuracy': model_results['accuracy'],
            'Precision': model_results['precision'],
            'Recall': model_results['recall'],
            'F1-Score': model_results['f1_score'],
            'ROC-AUC': model_results['roc_auc'] or np.nan
        })

    metrics_df = pd.DataFrame(metrics_data)
    metrics_df = metrics_df.sort_values('Accuracy', ascending=False)

    display(Markdown("### –¢–∞–±–ª–∏—Ü–∞ –º–µ—Ç—Ä–∏–∫ –º–æ–¥–µ–ª–µ–π"))
    display(metrics_df.style.background_gradient(subset=['Accuracy', 'F1-Score'], cmap='YlOrBr'))

    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
    fig, axes = plt.subplots(2, 3, figsize=(15, 10))
    fig.suptitle('–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π', fontsize=16, fontweight='bold')

    metrics_to_plot = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC']
    for idx, metric in enumerate(metrics_to_plot):
        ax = axes[idx // 3, idx % 3]
        if metric in metrics_df.columns:
            bars = ax.bar(metrics_df['Model'], metrics_df[metric],
                         color=['#3498db', '#2ecc71', '#e74c3c'])
            ax.set_title(metric, fontsize=12, fontweight='bold')
            ax.set_ylabel('Score')
            ax.set_ylim(0, 1)
            ax.tick_params(axis='x', rotation=45)

            # –î–æ–±–∞–≤–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –Ω–∞ —Å—Ç–æ–ª–±—Ü—ã
            for bar in bars:
                height = bar.get_height()
                ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                       f'{height:.3f}', ha='center', va='bottom', fontsize=9)

    # –°–∫—Ä—ã–≤–∞–µ–º –ø—É—Å—Ç–æ–π subplot
    axes[1, 2].axis('off')
    plt.tight_layout()
    plt.show()

# %% [markdown]
# ## 4. –ê–Ω–∞–ª–∏–∑ –º–∞—Ç—Ä–∏—Ü –æ—à–∏–±–æ–∫

# %%
if 'results' in locals():
    fig, axes = plt.subplots(1, len(results), figsize=(5*len(results), 4))
    if len(results) == 1:
        axes = [axes]

    fig.suptitle('–ú–∞—Ç—Ä–∏—Ü—ã –æ—à–∏–±–æ–∫', fontsize=16, fontweight='bold')

    for idx, (model_name, model_results) in enumerate(results.items()):
        ax = axes[idx]
        cm = np.array(model_results['confusion_matrix'])

        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                   xticklabels=['Human', 'AI'],
                   yticklabels=['Human', 'AI'],
                   cbar=False, ax=ax)

        ax.set_title(f'{model_name.replace("_", " ").title()}', fontweight='bold')
        ax.set_xlabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å', fontsize=10)
        ax.set_ylabel('–ò—Å—Ç–∏–Ω–Ω—ã–π –∫–ª–∞—Å—Å', fontsize=10)

    plt.tight_layout()
    plt.show()

# %% [markdown]
# ## 5. –ê–Ω–∞–ª–∏–∑ –ø—Ä–∏–º–µ—Ä–æ–≤ –æ—à–∏–±–æ–∫

# %%
# –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
try:
    test_data = pd.read_csv('../data/processed/test.csv')

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
    best_model_name = metrics_df.iloc[0]['Model'].replace(' ', '_').lower()
    best_predictions = results[best_model_name]['predictions']

    test_data['prediction'] = best_predictions
    test_data['correct'] = test_data['label'] == test_data['prediction']

    display(Markdown(f"### –ê–Ω–∞–ª–∏–∑ –æ—à–∏–±–æ–∫ –¥–ª—è –º–æ–¥–µ–ª–∏: {best_model_name.title()}"))

    # –ü—Ä–∏–º–µ—Ä—ã –æ—à–∏–±–æ–∫
    errors = test_data[~test_data['correct']]
    print(f"–í—Å–µ–≥–æ –æ—à–∏–±–æ–∫: {len(errors)} ({len(errors)/len(test_data)*100:.1f}%)")

    # –¢–∏–ø—ã –æ—à–∏–±–æ–∫
    false_ai = errors[(errors['label'] == 0) & (errors['prediction'] == 1)]
    false_human = errors[(errors['label'] == 1) & (errors['prediction'] == 0)]

    print(f"–õ–æ–∂–Ω—ã–µ —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏—è (Human ‚Üí AI): {len(false_ai)}")
    print(f"–ü—Ä–æ–ø—É—Å–∫–∏ (AI ‚Üí Human): {len(false_human)}")

    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–∏–º–µ—Ä—ã –æ—à–∏–±–æ–∫
    display(Markdown("#### –ü—Ä–∏–º–µ—Ä –ª–æ–∂–Ω–æ–≥–æ —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏—è (Human —Ç–µ–∫—Å—Ç –ø–æ–º–µ—á–µ–Ω –∫–∞–∫ AI):"))
    if len(false_ai) > 0:
        sample = false_ai.iloc[0]
        print(f"–¢–µ–∫—Å—Ç: {sample['text'][:200]}...")
        print(f"–ò—Å—Ç–æ—á–Ω–∏–∫: {sample['source']}")

    display(Markdown("#### –ü—Ä–∏–º–µ—Ä –ø—Ä–æ–ø—É—Å–∫–∞ (AI —Ç–µ–∫—Å—Ç –ø–æ–º–µ—á–µ–Ω –∫–∞–∫ Human):"))
    if len(false_human) > 0:
        sample = false_human.iloc[0]
        print(f"–¢–µ–∫—Å—Ç: {sample['text'][:200]}...")
        print(f"–ò—Å—Ç–æ—á–Ω–∏–∫: {sample['source']}")

except Exception as e:
    print(f"Error loading test data: {e}")

# %% [markdown]
# ## 6. –í—ã–≤–æ–¥—ã –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏

# %%
if 'results' in locals():
    display(Markdown("## üìä –ò–¢–û–ì–ò –ò –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò"))

    best_model = metrics_df.iloc[0]

    display(Markdown(f"### –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å: **{best_model['Model']}**"))
    display(Markdown(f"- –¢–æ—á–Ω–æ—Å—Ç—å (Accuracy): **{best_model['Accuracy']:.3f}**"))
    display(Markdown(f"- F1-Score: **{best_model['F1-Score']:.3f}**"))

    # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
    if best_model['Accuracy'] > 0.85:
        display(Markdown("### ‚úÖ –û—Ç–ª–∏—á–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç!"))
        display(Markdown("–ú–æ–¥–µ–ª—å –≥–æ—Ç–æ–≤–∞ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é –≤ production."))
    elif best_model['Accuracy'] > 0.75:
        display(Markdown("### ‚ö†Ô∏è –•–æ—Ä–æ—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç"))
        display(Markdown("–ú–æ–∂–Ω–æ —É–ª—É—á—à–∏—Ç—å —á–µ—Ä–µ–∑:"))
        display(Markdown("1. –ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫—É –∫–ª–∞—Å—Å–æ–≤"))
        display(Markdown("2. –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏"))
        display(Markdown("3. –ù–∞—Å—Ç—Ä–æ–π–∫—É –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤"))
    else:
        display(Markdown("### ‚ùå –¢—Ä–µ–±—É–µ—Ç—Å—è —É–ª—É—á—à–µ–Ω–∏–µ"))
        display(Markdown("–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è:"))
        display(Markdown("1. –£–≤–µ–ª–∏—á–∏—Ç—å –æ–±—ä–µ–º –¥–∞–Ω–Ω—ã—Ö"))
        display(Markdown("2. –ü–µ—Ä–µ—Å–º–æ—Ç—Ä–µ—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–∏"))
        display(Markdown("3. –ü–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–≤—ã–µ –ø–æ–¥—Ö–æ–¥—ã"))

    display(Markdown("### üìà –°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏:"))
    display(Markdown("1. –ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –Ω–∞ –Ω–æ–≤—ã—Ö, unseen –¥–∞–Ω–Ω—ã—Ö"))
    display(Markdown("2. –°–æ–∑–¥–∞—Ç—å API –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π"))
    display(Markdown("3. –ü–µ—Ä–µ–π—Ç–∏ –∫ –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–≤—ã–º –º–æ–¥–µ–ª—è–º (CNN, LSTM)"))