# TextDeepFakeGuard

**Hybrid Detection of Synthetic Text with Stylometry, Transformers, and Robustness Analysis**

ğŸ“Œ *Research & applied project on detecting AI-generated (deepfake) text using classical ML, neural networks, transformers, ensembles, and explainability.*

---

## ğŸ” Problem Statement
The development of large language models has led to the emergence of a large number of AI texts that need to be distinguished from human ones. This can be useful in education, journalism, social media, cybersecurity, and more. For this purpose, there are various methods of artificial intelligence and machine learning that allow, using different data, to detect such texts with varying degrees of effectiveness. This repository contains several models that demonstrate varying degrees of effectiveness depending on the set of texts and its size. Below is a readme.md file that will help with the configuration and use of these models.

**Goal:** build a robust system for detecting synthetic text that:
- works across languages (RU / EN),
- generalizes to unseen LLMs,
- remains stable under paraphrasing and style changes,
- is explainable and suitable for online deployment.

---
## ğŸ§  Methods Overview
We evaluate and compare multiple families of models:

### 1. Classical ML (Baselines)
- Logistic Regression
- Linear SVM
- Random Forest

**Features:**
- TF-IDF (word- and character-level n-grams)
- Stylometric features (lexical diversity, entropy, punctuation ratios, POS statistics)

---

### 2. Neural Networks
- Character-level CNN
- BiLSTM / GRU with attention

Used to capture sequential and morphological patterns typical for synthetic text.

---

### 3. Transformer Models (Fine-tuning)
- BERT
- RuBERT
- XLM-R
- mT5

Models are fine-tuned for binary classification (human vs synthetic text) and evaluated for generalization and inference speed.

---

## ğŸ”¥ Proposed Method: HSSE
### **Hybrid Stylometricâ€“Semantic Ensemble**

The core contribution of this project is a hybrid ensemble approach combining complementary signals:

**Components:**
1. **Semantic probability** â€” output of a fine-tuned transformer
2. **Stylometric score** â€” classical ML on handcrafted linguistic features
3. **Perplexity gap** â€” difference between autoregressive LM and masked LM perplexity
4. **Stability score** â€” prediction variance under:
   - paraphrasing
   - back-translation
   - style transformation

**Final decision:** stacking via a meta-classifier (Logistic Regression / LightGBM).

This design improves robustness and interpretability while maintaining competitive accuracy.

---

## ğŸ§ª Experimental Setup
- Data split: train / validation / test
- Cross-validation for classical models
- Evaluation on unseen LLM-generated texts

### Metrics
- Accuracy
- Precision / Recall
- F1-score
- ROC-AUC

### Additional Evaluations
- **Robustness score** (Î”F1 after text transformations)
- **Generalization gap** (seen vs unseen generators)
- **Latency** (ms per text)
- **Model size & memory footprint**

---

## ğŸ” Explainability (XAI)
- SHAP values for ML and ensemble models
- Attention visualization for transformers
- Token-level importance heatmaps
- LIME for local explanations

---

## âš¡ Inference & Deployment
- Batch and real-time inference benchmarks
- REST API prototype
- Streamlit demo application

---

## ğŸ“ Repository Structure
```text
text-deepfake-guard/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â””â”€â”€ processed/
â”‚
â”œâ”€â”€ features/
â”‚   â”œâ”€â”€ stylometric.py
â”‚   â””â”€â”€ embeddings.py
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ ml/
â”‚   â”œâ”€â”€ neural/
â”‚   â”œâ”€â”€ transformers/
â”‚   â””â”€â”€ ensemble/
â”‚
â”œâ”€â”€ experiments/
â”‚   â”œâ”€â”€ evaluation.ipynb
â”‚   â””â”€â”€ robustness.ipynb
â”‚
â”œâ”€â”€ xai/
â”‚   â””â”€â”€ shap_analysis.ipynb
â”‚
â”œâ”€â”€ inference/
â”‚   â”œâ”€â”€ api.py
â”‚   â””â”€â”€ benchmark.py
â”‚
â”œâ”€â”€ demo/
â”‚   â””â”€â”€ streamlit_app.py
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ methodology.pdf
â”‚   â””â”€â”€ results.md
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸš€ How to Run
```bash
pip install -r requirements.txt
python inference/api.py
```

---

## ğŸ“Œ Author
**Alexey Staroverov, Nikita Petrov**  
BSc Applied Mathematics, HSE University  
Interests: NLP, ML, AI Safety, Robustness, Explainability

---

## ğŸ“„ Citation
If you use this project in academic work:
```
@misc{textdeepfakeguard2025,
  title={TextDeepFakeGuard: Hybrid Detection of Synthetic Text},
  author={Staroverov Alexey, Petrov Nikita},
  year={2026}
}
```
